# 클러스터 멤버십

카프카는 현재 클러스터의 멤버인 브로커들의 목록을 유지하기 위해 아파치 주키퍼를 사용한다. <br>
각 브로커는 브로커 설정 파일에 정의되었거나, 자동으로 생성된 고유한 식별자를 가진다.

브로커 프로세스는 시작될 때마다 주키퍼에 Ephemeral 노드의 형태로 ID를 등록한다. <br>
만약 동일한 ID를 가진 다른 브로커를 시작한다면, 에러가 발생한다. <br>
만약 특정한 브로커가 완전히 유실되어 동일한 ID를 가진 새로운 브로커를 투입할 경우, 곧바로 클러스터에서 유실된 브로커의 자리를 대신해서 이전 브 로커의 토픽과 파티션들을 할당받는다.


<br>
<hr>

# 컨트롤러

컨트롤러는 일반적인 카프카 브로커의 기능에 더해서 파티션 리더를 선출하는 역할을 추가적으로 맡는다. <br>
클러스터에서 가장 먼저 시작되는 브로커는 주키퍼의 /controller에 Ephemeral 노드를 생성함으로써 컨트롤러가 된다. <br>
브로커들은 주키퍼의 컨트롤러 노드에 뭔가 변동이 생겼을 때 알림을 받기 위해서 이 노드에 와치를 설정한다. <br>
**이렇게 함으로써 우리는 클러스터 안에 한 번에 단 한 개의 컨트롤러만 있도록 보장할 수 있다.**

컨트롤러 브로커가 멈추거나 주키퍼와의 연결이 끊어질 경우, 이 Ephemeral 노드는 삭제된다. *(주키퍼 클라이언트가 zookeeper.session.timeout.ms에 설정된 값보다 더 오랫 동안 주키퍼에 하트비트를 전송하지 않는 것도 해당)* <br>
이런 경우, 클러스터 안의 다른 브로커들은 주키퍼에 설정된 와치를 통해 컨트롤러가 없어졌다는 것을 알아차리게 되고, 주키퍼에 컨트롤러 노드를 생성하려고 시도한다. <br>
가장 먼저 새로운 노드를 생성하는 데 성공한 브로커가 다음 컨트롤러가 되며, 다른 브로커들은 "노드가 이미 존재함" 예외를 받고 새 컨트롤러 노드에 대한 와치를 다시 생성하게 된다.

브로커는 새로운 컨트롤러가 선출될 때마다 주키퍼의 **conditional increment** 연산에 의해 증가된 에포크 값을 전달받고, <br>
브로커는 현재 컨트롤러의 에포크 값을 알고 있기 때문에. 만약 더 낮은 (예전) 에포크 값을 가진 컨트롤러로부터 메시지를 받을 경우 무시한다.

## KRaft: 카프카의 새로운 래프트 기반 컨트롤러

카프카 3.3부터 주키퍼 기반 컨트롤러로부터 탈피해서 raft 기반 컨트롤러 쿼럼으로 옮겨 갔다. <br>
이에 따라 최신 버전의 카프카 클러스터는 전통적인 주키퍼 기반 컨트롤러와 KRaft 기반 컨트롤러 둘 중 하나와 함께 실행될 수 있을 것이다. <br>
컨트롤러를 교체한 이유는 아래와 같다.

- 컨트롤러가 주키퍼에 메타데이터를 쓰는 작업은 동기적으로 이루어지지만, 브로커 메시지를 보내는 작업은 비동기적으로 이루어진다. (주키퍼로부터 업데이트를 받는 과정 역시 비동기) 그렇기 때문에 브로커, 컨트롤러, 주키퍼 간에 메타데이터 불일치가 발생할 수 있으며, 잡아내기도 어렵다.
- 컨트롤러가 재시작될 때마다 주키퍼로부터 모든 브로커와 파티션에 대한 메타데이터를 읽어와 야 한다.
- 메타데이터 소유권 관련된 내부 아키텍처는 그리 좋지 못하다. 즉, 어떤 작업은 컨트롤러가 하고, 다른 건 브로커가 하고, 나머지는 주키퍼가 직접 한다.
- 주키퍼는 그 자체로 분산 시스템이며, 카프카와 마찬가지로 운영을 위해서는 어느 정도 기반 지식이 있어야 한다.

(참고)[https://hoing.io/archives/4029]

<br>
<hr>

# 복제

복제가 중요한 이유는 개별적인 노드에 필연적으로 장애가 발생할 수밖에 없는 상황에서 카프카가 신뢰성과 지속성을 보장하는 방식이기 때문이다.

카프카에 저장되는 데이터는 토픽을 단위로 해서 조직화된다. 각 토픽은 1개 이상의 파티션으로 분할되며, 각 파티션은 다시 다수의 레플리카를 가질 수 있다. <br>
각각의 레플리 카는 브로커에 저장되는데, 대개 하나의 브로커는 수백 개, 수천 개의 레플리카를 저장한다.

- 리더 레플리카
  - 각 파티션에는 리더 역할을 하는 레플리카가 하나씩 있다.
  - 일관성을 보장하기 위해, 모든 쓰기 요청은 리더 레플리카로 주어진다.
  - 클라이언트들은 리더 레플리카나 팔로워로부터 레코드를 읽어올 수 있다.
  - 팔로워 레플리카가 리더 레플리카의 최신 상태를 유지하고 있는지를 확인한다.
- 팔로워 레플리카
  - 파티션에 속한 모든 레플리카 중에서 리더 레플리카를 제외한 나머지를 팔로워 레플리카라고 한다.
  - 별도로 설정을 잡아주지 않는 한, 팔로워는 클라이언트의 요청을 처리할 수 없다.
  - 이들이 주로 하는 일은 리더 레플리카로 들어온 최근 메시지들을 복제함으로써 최신 상태를 유지하는 것이다.
  - 만약 해당 파티션의 리더 레플리카에 크래쉬가 날 경우, 팔로워 레플리카 중 하나가 파티션의 새 리더 파티션으로 승격된다.

<br>
<hr>

# 요청 처리

카프카 브로커가 하는 일의 대부분은 클라이언트, 파티션 레플리카, 컨트롤러가 파티션 리더에게 보내는 요청을 처리하는 것이다.

브로커는 연결을 받는 각 포트별로 acceptor 스레드를 하나씩 실행시킨다. <br>
acceptor 스레드는 연결을 생성하고 들어온 요청을 프로세서 스레드(네트워크 스레드)에 넘겨 처리하도록 한다. <br>
네트워크 스레드는 클라이언트 연결로부터 들어온 요청들을 받아서 요청 큐에 넣고, 응답 큐에서 응답을 가져다 클라이언트로 보낸다.

<img width="460" alt="스크린샷 2023-09-28 오후 3 36 34" src="https://github.com/flataex/kafka-study/assets/87420630/0eba90b4-2fea-4a5d-830d-267222b7fb78">

일단 요청이 요청 큐에 들어오면, I/O 스레드가 요청을 가져와서 처리하는 일을 담당한다. <br>
일반적인 클라이언트 요청 유형은 다음과 같다.

- 쓰기 요청
  - 카프카 브로커로 메시지를 쓰고 있는 프로듀서가 보낸 요청
- 읽기 요청
  - 카프카 브로커로부터 메시지를 읽어오고 있는 컨슈머나 팔로워 레플리카가 보낸 요청
- 어드민 요청
  - 토픽 생성이나 삭제와 같이 메타데이터 작업을 수행중인 어드민 클라이언트가 보낸 요청

> 쓰기 요청과 읽기 요청 모두 파티션의 리더 레플리카로 전송되어야 한다. <br>
> 카프카의 클라이언트는 요청에 맞는 파티션의 리더를 맡고 있는 브로커에 쓰기나 읽기 요청을 전송할 책임을 진다.

> 카프카 클라이언트는 메타데이터 요청이라 불리는 또 다른 유형의 요청을 사용해서 토픽들에 어떤 파티션들이 있고, 각 파티션의 레플리카에는 무엇이 있으며, 어떤 레플리카가 리더인지를 명시하는 응답을 받을 수 있다. <br>
> 모든 브로커들이 이러한 정보를 포함하는 메타데이터 캐시를 가지고 있기 때문에 클라이언트는 어디로 요청을 보내야 하는지 알 수 있다.

<img width="460" alt="스크린샷 2023-09-28 오후 3 41 20" src="https://github.com/flataex/kafka-study/assets/87420630/3cef22cb-1212-4c16-9b6f-3fd258f6d0b5">

## 쓰기 요청

acks 설정 매개변수는 쓰기 작업이 성공한 것으로 간주되기 전 메시지에 대한 응답을 보내야 하는 브로커의 수를 가리킨다.

- acks=1: 리더만이 메시지를 받았을 때
- acks=all: 모든 인-싱크 레플리카들이 메시지를 받았을 때
- acks=0: 메시지가 보내졌을 때. 즉, 브로커의 응답을 기다리지 않음


파티션의 리더 레플리카를 가지고 있는 브로커가 해당 파티션에 대한 쓰기 요청을 받게 되면 몇 가지 유효성 검증부터 한다.

- 데이터를 보내고 있는 사용자가 토픽에 대한 쓰기 권한을 가지고 있는가?
- 요청에 지정되어 있는 acks 설정값이 올바른가? (0, 1, all만이 사용 가능)
- 만약 acks 설정값이 all로 잡혀 있을 경우, 메시지를 안전하게 쓸 수 있을 만큼 충분한 인-싱크 레플리카가 있는가?

그러고 나서 브로커는 새 메시지들을 로컬 디스크에 쓴다. *(리눅스의 경우 메시지는 파일시스템 캐시에 쓰여지는데, 이들이 언제 디스크에 반영될지에는 보장이 없다.)* <br>

## 읽기 요청

클라이언트는 브로커에 토픽, 파티션, 오프셋 목록에 해당하는 메시지들을 보내 달라는 요청을 보낸다. <br>
요청은 요청에 지정된 파티션들의 리더를 맡고 있는 브로커에 전송 되어야 하며, 클라이언트는 읽기 요청을 정확히 라우팅할 수 있도록 필요한 메타데이터에 대한 요청을 보내게 된다.

> 카프카는 클라이언트에게 보내는 메시지에 제로카피 최적화를 적용한다. <br>
> 파일에서 읽어 온 메시지들을 중간 버퍼를 거치지 않고 바로 네트워크 채널로 보내는 것이다. <br>
> 이 방식을 채택함으로써 데이터를 복사하고 메모리 상에 버퍼를 관리하기 위한 오버헤드가 사라지며, 결과적으로 성능이 향상된다.

<img width="625" alt="스크린샷 2023-09-28 오후 3 48 04" src="https://github.com/flataex/kafka-study/assets/87420630/4243e310-d78f-405b-90b1-7e3b77bff71e">

대부분의 클라이언트는 모든 인-싱크 레플리카에 쓰여진 메시지들만을 읽을 수 있다. <br>
이러한 작동의 이유는, 충분한 수의 레플리카에 복제가 완료되지 않은 메시지는 **불안전한** 것으로 간주된다. <br>
만약 리더에 크래시가 발생해서 다른 레플리카가 리더 역할을 이어받는다면, 이 메시지들은 더 이상 카프카에 존재하지 않게 된다. <br>
만약 클라이언트가 이렇게 리더에만 존재하는 메시지들을 읽을 수 있도록 한다면, 크래시 상황에서 일관성이 결여될 수 있는 것이다.

<img width="757" alt="스크린샷 2023-09-28 오후 3 50 36" src="https://github.com/flataex/kafka-study/assets/87420630/86bccd41-ec3c-46e8-9f28-be012b730c2e">

> 컨슈머가 매우 많은 수의 파티션들로부터 이벤트를 읽어오는 경우가 있다. <br>
> 이 경우 읽고자 하는 파티션의 전체 목록을 요청을 보낼 때마다 브로커에 전송하고, 다시 브로커는 모든 메타데이터를 돌려 보내는 방식은 매우 비효율적일 수 있다. <br>
> 읽고자 하는 파티션의 집합이나 여기에 연관된 메타데이터는 여간해서는 잘 바뀌지 않는 데다가, 많은 경우 리턴해야 할 메타데이터가 그렇게 많지도 않다. <br>
> 이러한 오버헤드를 최소화하기 위해 카프카는 **읽기 세션 캐시**를 사용한다. *(컨슈머는 읽고 있는 파티션의 목록과 그 메타데이터를 캐시하는 세션을 생성할 수 있다.)* <br>


<br>
<hr>

# 물리적 저장소

카프카의 기본 저장 단위는 파티션 레플리카이다. 파티션은 서로 다른 브로커들 사이에 분리될 수 없으며, 같은 브로커의 서로 다른 디스크에 분할 저장되는 것조차 불가능하다.

## 계층화된 저장소

계층화된 저장소 기능에서는 카프카 클러스터의 저장소를 로컬과 원격, 두 계층으로 나눈다. <br>
**로컬 계층**은 현재의 카프카 저장소 계층과 똑같이 로컬 세그먼트를 저장하기 위해 카프카 브로커의 로컬 디스크를 사용한다. <br>
**원격 계층**은 완료된 로그 세그먼트를 저장하기 위해 HDFS나 S3와 같은 전용 저장소 시스템을 사용한다.

사용자는 계층별로 서로 다른 보존 정책을 설정할 수 있다. <br>
로컬 저장소가 리모트 계층 저장소에 비해 훨씬 비싼 것이 보통이므로 로컬 계층의 보존 기한은 몇 시간 이하로 설정하고, 원격 계층의 보존 기한은 그보다 길게(며칠이나 몇 달로) 설정하는 것이다.

로컬 저장소는 원격 저장소에 비해 지연이 훨씬 짧다. <br>
지연에 민감한 애플리케이션들은 로컬 계층에 저장되어 있는 최신 레코드를 읽어오는 만큼, 데이터를 전달하기 위해 페이지 캐시를 효율적으로 활용하는 카프카의 메커니즘에 의해 문제없이 작동한다. <br>
빠진 처리 결과를 메꾸는 작업이나 장애에서 복구되고 있는 애플리케이션들은 로컬 계층에 있는 것보다 더 오래된 데이터를 필요로 하는 만큼 원격 계층에 있는 데이터가 전달된다.

**계층화된 저장소 기능은 무한한 저장 공간, 더 낮은 비용, 탄력성뿐만 아니라 오래 된 데이터와 실시간 데이터를 읽는 작업을 분리시키는 기능이 있다.**

## 파일 관리

카프카 운영자는 각각의 토픽에 대해 보존 기한을 설정할 수 있다. <br>
기본적으로, 각 세그먼트는 1GB의 데이터 혹은 최근 1주일치의 데이터 중 적은 쪽만큼을 저장한다. 카프카가 파티션 단위로 메시지를 쓰는 만큼 각 세그먼트 한도가 다 차면 세그먼트를 닫고 새 세그먼트를 생성한다.

현재 쓰여지고 있는 세그먼트를 **액티브 세그먼트**라고 하고, 액티브 세그먼트는 어떠한 경우에도 삭제되지 않는다. **(보존 기한 영향 안받음)**









